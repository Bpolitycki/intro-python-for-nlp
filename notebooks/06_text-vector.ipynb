{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce93e05a",
   "metadata": {},
   "source": [
    "# Texte im Vektorraum\n",
    "\n",
    "Bisher haben wir nur unmittelbar mit den Textdaten selbst gearbeitet und mit einfachen Mitteln erste statistische Untersuchungen durchgeführt. Die Inhalte dieser Übungseinheit schließen unmittelbar hieran an. Im Fokus steht jedoch nicht länger die ‚direkte‘ Arbeit mit den Texten selbst, sondern die Umwandlung der Textdaten in Vektoren, die sich wesentlich effizienter verarbeiten und auswerten lassen. \n",
    "\n",
    "## Externe Bibliotheken\n",
    "\n",
    "Im Python-Ökosystem hat sich rund um die Themengebiete das Natural Language Processing wie auch der Datawissenschaft (*Data Science*) in den letzten Jahren eine Vielzahl von Bibliotheken entwickelt. An dieser Stelle werden nachfolgend einige dieser Bibliotheken verwendet, dazu zählen:\n",
    "\n",
    "* [Pandas](https://pandas.pydata.org) zur Datenspeicherung / -auswertung\n",
    "* [NLTK](https://www.nltk.org)\n",
    "* [scikit-learn](https://scikit-learn.org/stable/)\n",
    "\n",
    "Diese Bibliotheken sind standardmäßig Teil der [Anaconda-Distribution](https://www.anaconda.com).\n",
    "\n",
    "## Texte als Vektoren\n",
    "\n",
    "Die Umwandlung von Textdaten in Vektoren spielt eine zentrale Rolle im Bereich der digitalen Textanalyse. Im Kern besteht diese Notwendigkeit aus dem fundamentalen Unterschied zwischen der Art und Weise, wie Menschen und Maschinen Informationen verarbeiten. Menschen können Text auf natürliche Weise interpretieren und dessen Bedeutung und Kontext erfassen – Maschinen nicht.\n",
    "\n",
    "Die zentralen Informationen eines Textes müssen notwendigerweise auf eine Art und Weise umgewandelt werden, die von Maschinen verstanden und analysiert werden kann. Typischerweise werden die Textdaten dabei in Vektoren umgewandelt bzw. als solche repräsentiert. Idealisiert könnte dies für einen Beispielsatz wie `Die Bundesregierung hat sich auf einen Kompromiss geeinigt.` folgendermaßen aussehen:\n",
    "\n",
    "```\n",
    "0\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "5\n",
    "6\n",
    "7\n",
    "```\n",
    "\n",
    "Oder als einfache Liste in Python:\n",
    "\n",
    "```python\n",
    "[0, 1, 2, 3, 4, 5, 6, 7]\n",
    "```\n",
    "\n",
    "Durch diese Transformation wird jeder Text, sei es ein Wort, ein Satz oder ein ganzer Absatz, in eine Reihe von Zahlen (also einen Vektor) umgewandelt. Jede Zahl in diesem Vektor repräsentiert ein Merkmal (Feature) des Textes, wie etwa seine Häufigkeit in einem Dokument, seine Position in einem Satz – wie im Beispiel – oder seine semantische Ähnlichkeit mit anderen Wörtern. \n",
    "\n",
    "**Weiterführende Literatur**\n",
    "\n",
    "- Hirschle, Jochen. Deep Natural Language Processing: Einstieg in Word Embedding, Sequence-to-Sequence-Modelle Und Transformers Mit Python – insbesondere Kapitel 2.1 und 4.1.\n",
    "- McGillivray, Barbara, und Gábor Mihály Tóth. Applying Language Technology in Humanities Research: Design, Application, and the Underlying Logic. Springer International Publishing, 2020. DOI.org (Crossref), https://doi.org/10.1007/978-3-030-46493-6 – insbesondere Kapitel 5.\n",
    "\n",
    "## Bag Of Words \n",
    "\n",
    "Das sog. *Bag of Words* (BoW) Modell ist eine der grundlegenden und weit verbreiteten Methoden zur Textvektorisierung. Obwohl es sich hierbei um recht simple Repräsentation der Textdaten handelt, lässt es es für eine Vielzahl von Anwendungen verwenden. Dazu zählen etwa Dokumentenklassifizierung, Sentiment-Analyse oder auch Information Retrieval.\n",
    "\n",
    "Das BoW-Modell behandelt einen Text als ‚Beutel‘ von Wörtern, ohne dabei auf deren Reihenfolge zu achten. Die Zählung von Häufigkeiten ist wichtiger als die Reihenfolge der Worte im Text. Um ein BoW-Repräsentation zu erstellen, wird zunächst eine Art Wörterbuch aller einzigartigen Wörter in allen Dokumenten oder Texten, die wir analysieren wollen, erstellt (das sog. Vokabular). Jedes einzigartige Wort bekommt einen Index zugewiesen. Dann wird jedes Dokument in einen Vektor umgewandelt, der die Häufigkeit jedes Worts aus dem Wörterbuch im Dokument darstellt. Wenn ein Wort im Dokument vorkommt, wird der entsprechende Index im Vektor erhöht.\n",
    "\n",
    "Angenommen wir haben die drei Beispieldokumente:\n",
    "\n",
    "1. Die Bundesregierung hat sich auf einen Kompromiss geeinigt.\n",
    "2. Die Opposition ist mit dem Kompromiss nicht zufrieden.\n",
    "3. Der Bundeskanzler hat sich für den Kompromiss eingesetzt.\n",
    "\n",
    "Dann sieht die zugehörige BoW-Repräsentation (bei Vernachlässigung von Groß- und Kleinschreibung) in tabellarischer Form folgendermaßen aus:\n",
    "\n",
    "|        | die | bundesregierung | hat | sich | auf | einen | kompromiss | geeinigt | opposition | ist | mit | dem | nicht | zufrieden | bundeskanzler | für | den | eingesetzt |\n",
    "|--------|-----|-----------------|-----|------|-----|-------|------------|----------|------------|-----|-----|-----|------|-----------|---------------|-----|-----|------------|\n",
    "| Text 1 |   1 |               1 |   1 |    1 |   1 |     1 |          1 |        1 |          0 |   0 |   0 |   0 |    0 |         0 |             0 |   0 |   0 |          0 |\n",
    "| Text 2 |   1 |               0 |   0 |    0 |   0 |     0 |          1 |        0 |          1 |   1 |   1 |   1 |    1 |         1 |             0 |   0 |   1 |          0 |\n",
    "| Text 3 |   0 |               0 |   1 |    1 |   0 |     0 |          1 |        0 |          0 |   0 |   0 |   0 |    0 |         0 |             1 |   1 |   1 |          1 |\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13fce905",
   "metadata": {},
   "source": [
    "## Inhalt dieser Übung\n",
    "\n",
    "Nachfolgend soll eine solche Textrepräsentation für tatsächliche Dokumente erstellt werden. Als Beispiel dienen die Reden der 20. Legislaturperiode des deutschen Bundestages. Führen Sie zum Download der Beispieldaten zunächst die nächste Zelle aus. Eine Einführung zum Format der Daten und den dort enthaltenen Informationen finden Sie in [vorherigen Übungseinheit](./05_working-with-texts.ipynb). \n",
    "\n",
    "Ziel dieser Einheit ist es die Grundprinzipien der Textaufbereitung und Repräsentation mithilfe von BoW zu erproben und basierend hierauf einfache Berechnungen durchzuführen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ff43a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "\n",
    "sciebo_download = \"https://uni-wuppertal.sciebo.de/s/OVSgl4lSy4AxBgt/download\"\n",
    "http_response = urlopen(sciebo_download)\n",
    "with ZipFile(BytesIO(http_response.read())) as zip_file:\n",
    "    zip_file.extractall(path='.')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4649f561",
   "metadata": {},
   "source": [
    "Zur einfachen Aufbereitung der Texte sei folgende Funktion gegeben, die unerwünschte Zeichen entfernt und den Text in durchgängige Kleinschreibung konvertiert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867d43ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_unwanted_chars_and_lower(text_input):\n",
    "    \"\"\"Die Methode `sub()` nimmt drei Parameter entgegen:\n",
    "    1. Das Suchmuster \n",
    "    2. Womit das Muster ersetzt werden soll\n",
    "    3. Den Textinhalt, auf dem diese Operation ausgeführt wird\n",
    "    \"\"\"\n",
    "    # Entferne Zeilenumbrüche, Tabs und Return-Carriage\n",
    "    text_cleaned = re.sub('[\\n\\t\\r]', ' ', text_input)\n",
    "    # Entferne alle Sonderzeichen, die nicht Buchstaben oder Leerzeichen sind\n",
    "    text_cleaned = re.sub(r'[\\.:;!?,–\\d]', '', text_cleaned)\n",
    "    # Entferne alle doppelten Leerzeichen\n",
    "    text_cleaned = re.sub(r'\\s+', ' ', text_cleaned)\n",
    "    return text_cleaned.lower()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61be89eb",
   "metadata": {},
   "source": [
    "## Planung des Vorgehens \n",
    "\n",
    "Für die Untersuchung der Plenarprotokolle JSON-Daten zunächst eingelesen und verarbeitet werden. Ein typischer Arbeitsablauf könnte folgendermaßen aussehen:\n",
    "\n",
    "1. Erstellen einer Liste aller Dateien, die eingelesen werden sollen.\n",
    "2. Erstellen eines leeren Pandas-DataFrames mit den relevanten Spalten\n",
    "3. Einlesen der Dateien in einer Schleife\n",
    "4. Extrahieren der benötigten Informationen\n",
    "5. Hinzufügen zum DataFrame\n",
    "6. Zwischenspeichern\n",
    "7. Bereinigung (Entfernen von Stoppwörtern, Lemmatisierung etc.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "08cbb52d",
   "metadata": {},
   "source": [
    "### 1. Dateiliste anlegen\n",
    "\n",
    "In Python kann das `glob`-Modul verwendet werden, um eine Liste von Dateien basierend auf einem bestimmten Muster oder einem Pfadmuster zu erstellen. Das `glob`-Modul unterstützt das Matching von Dateinamen mit Wildcards wie `*` und `?`. \n",
    "\n",
    "1. Importiere das `glob`-Modul:\n",
    "```python\n",
    "import glob\n",
    "```\n",
    "\n",
    "2. Verwende die `glob.glob()`-Funktion, um eine Liste von Dateien basierend auf einem Muster zu erstellen. Das Muster kann ein Dateiname oder ein Pfadmuster sein. Zum Beispiel:\n",
    "```python\n",
    "file_list = glob.glob('Pfad_zum_Ordner/*.txt')\n",
    "```\n",
    "Dieses Beispiel sucht nach allen Dateien mit der Erweiterung `.txt` im angegebenen Verzeichnis. \n",
    "\n",
    "Das Muster kann auch wildcards verwenden. Beispielsweise kann `'*.xml'` verwendet werden, um alle Dateien mit der Erweiterung `.xml` zu erfassen, oder `'file_?.txt'`, um Dateien mit Namen wie `file_1.txt`, `file_2.txt`, usw. zu erfassen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e001bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Finden aller Dateien im Ordner `pp20`, die auf `.json` enden\n",
    "files = glob.glob('./pp20/*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472fce7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anzeige der ersten 3 Dateinamen\n",
    "files[:3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4757e5b3",
   "metadata": {},
   "source": [
    "### 2. Arbeit mit Pandas\n",
    "\n",
    "Pandas ist eine leistungsstarke und flexible Open-Source-Datenanalyse- und -manipulationsbibliothek für Python. Sie bietet Datenstrukturen und Funktionen, die notwendig sind, um numerische Tabellen zu manipulieren und zu analysieren. Pandas ermöglicht das Laden, Vorbereiten, Manipulieren und Analysieren von Daten in Python. Der Grundbaustein sind sog. `DataFrames`, in denen die Daten in einer Tabellenform abgelegt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7416795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import der Bibliothek als Alias `pd`\n",
    "import pandas as pd\n",
    "\n",
    "# Definition von Anzeigeoptionen\n",
    "pd.set_option('max_colwidth', 150)\n",
    "\n",
    "# Definition von Spaltennamen für einen ersten DataFrame\n",
    "columns = ['speech_id', 'file_name', 'speech']\n",
    "\n",
    "# Anlegen eines initialen DataFrames ohne Inhalt\n",
    "df_initial = pd.DataFrame(columns=columns)\n",
    "df_initial"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34200b47",
   "metadata": {},
   "source": [
    "3. / 4. Einlesen der Dateien und Extraktion der Information\n",
    "\n",
    "Nachfolgend wird eine Funktion `read_speeches_to_df` definiert, die alle Reden mit zugehöriger ID ausliest und in einem `DataFrame` hinzufügt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63eec9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_speeches_to_df(filename, df):\n",
    "    with open(filename, 'r', encoding='UTF-8') as file:\n",
    "    # Verarbeitung der geöffneten JSON-Datei mit der Funktion `load`\n",
    "        content = json.load(file)\n",
    "    \n",
    "    filename_without_slash = filename.split('/')[-1]\n",
    "    \n",
    "    for speech in content['speeches']:\n",
    "        # Extraktion der Informationen und Speicherung in einer Liste\n",
    "        doc = [speech['id'], filename_without_slash, speech['text']]\n",
    "        # Hinzufügen zum DataFrame als neue Zeile\n",
    "        df.loc[len(df)] = doc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74b4b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anwendung am Beispiel des ersten Protokolls in der Dateiliste\n",
    "read_speeches_to_df(files[0], df_initial)\n",
    "# Ausgabe des DataFrames\n",
    "df_initial"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7fff120a",
   "metadata": {},
   "source": [
    "### 6. Zwischenspeichern (optional)\n",
    "\n",
    "Jeder `DataFrame` kann problemlos in verschiedene Dateiformate überführt werden. Pandas bietet hierzu verschiedene Methoden wie `to_csv` an. Weitere Informationen finden sich in der Dokumentation: [Pandas Docs](https://pandas.pydata.org/docs/reference/frame.html)\n",
    "\n",
    "### 7. Bereinigung / Aufbereitung\n",
    "\n",
    "Bevor wir nun die Texte in ein Bow-Modell umwandeln können, führen wir zunächst einige Arbeitsschritte zur Bereinigung durch. Zunächst sollen unerwünschte Zeichen wie etwa `\\n` für Zeilenumbrüche entfernt werden, bevor dann häufig vorkommende Worte, die keinen besonderen Informationswert haben (Stoppwörter), ebenfalls entfernt werden. Dafür wird zunächst eine Liste von Stoppwörtern heruntergeladen und importiert. Anschließend wird eine Funktion zum Entfernen der Stoppworte definiert und diese wird gemeinsam mit der zuvor definierten Bereinigungsfunktion zeilenweise auf den Inhalt der Spalte `speech`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1e1bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import download\n",
    "from nltk.corpus import stopwords\n",
    "# Download der Stoppwortliste\n",
    "download('stopwords')\n",
    "# Laden der Stoppworte\n",
    "ger_stops = stopwords.words('german')\n",
    "ger_stops[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e854437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(text_input, stop_words):\n",
    "    text_tokenized = text_input.split(' ')\n",
    "    cleaned_text = []\n",
    "    \n",
    "    for word in text_tokenized:\n",
    "        if word in stop_words:\n",
    "            continue\n",
    "        cleaned_text.append(word)\n",
    "    \n",
    "    return \" \".join(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6e4e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_initial['speech'] = df_initial.speech.apply(\n",
    "        lambda row: remove_stop_words(remove_unwanted_chars_and_lower(row), ger_stops)\n",
    "    )\n",
    "df_initial"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ddca2356",
   "metadata": {},
   "source": [
    "### Erstellen eines BoW-Modells mit scikit-learn\n",
    "\n",
    "Die aufbereiteten Inhalte der Spalte `speech` lassen sich nun eine BoW-Repräsentation überführen. Dafür wird hier die Klasse `CountVectorizer` aus scikit-learn verwendet.\n",
    "\n",
    "Hierauf wird eine Methode `fit_transform()` aufgerufen, der die Textinhalte übergeben werden. Diese Methode führt folgende Operatione aus:\n",
    "\n",
    "1. Zunächst wird das Vokabular erstellt (`fit`)\n",
    "2. Anschließend wird die Matrix mit den Zählungen für jedes Dokument erstellt (`transform`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac01163",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "data_cv = cv.fit_transform(df_initial.speech)\n",
    "data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names_out())\n",
    "data_dtm.index = df_initial.speech_id\n",
    "data_dtm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f051f2d",
   "metadata": {},
   "source": [
    "Die BoW-Repräsentation lässt sich nun für weitere Untersuchungen nutzen:\n",
    "\n",
    "- Berechnung der relativen Dokumenthäufigkeit der einzelnen Worte, um herauszufinden, welche Worte immer wieder verwendet werden\n",
    "- Vergleiche von Dokumenten (der Reden) bspw. durch Berechnung der Cosinus-Distanz"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "87ad3ece",
   "metadata": {},
   "source": [
    "## Übungsaufgabe \n",
    "\n",
    "1. Erstellen Sie einen leeren DataFrame mit den Spalten speech_id, file_name und speech und weisen Sie diesen der Variable `df_speeches_raw` zu\n",
    "2. Lesen Sie *alle* Dateien mithilfe der Funktion `read_speeches_to_df` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e3d97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['speech_id', 'file_name', 'speech', ]\n",
    "df_speeches_raw = pd._____(columns=_____)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf81bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    assert isinstance(df_speeches_raw, pd.DataFrame)\n",
    "    for col_name in columns:\n",
    "        assert col_name in df_speeches_raw.columns\n",
    "    print(\"DataFrame korrekt erstellt!\")\n",
    "except AssertionError:\n",
    "    print(\"Die Variable `df_speeches_raw` ist nicht vom Typ `pd.DataFrame`\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd93776a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in _____:\n",
    "    read_speeches_to_df(file, _____)\n",
    "df_speeches_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e218e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    assert len(df_speeches_raw) == 11293\n",
    "    print(\"DataFrame korrekt erstellt!\")\n",
    "except AssertionError:\n",
    "    print(\"Die Variable `df_speeches_raw` enthält nicht die korrekte Anzahl an Zeilen\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c2755553",
   "metadata": {},
   "source": [
    "3. Erstellen Sie eine Bow-Repräsentation – führen vorab jedoch die Funktionen zur Bereinigung durch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fac6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_speeches_raw['speech'] = df_speeches_raw._____.apply(\n",
    "        lambda row: _____(remove_unwanted_chars_and_lower(row), _____)\n",
    "    )\n",
    "df_speeches_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df587e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    assert '.' not in df_speeches_raw.loc[0].speech\n",
    "    print(\"Bereinigung erfolgreich!\")\n",
    "except AssertionError:\n",
    "    print(\"Die Bereinigung der Redebeiträge war nicht erfolgreich\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd4d309",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_cv = CountVectorizer()\n",
    "data_cv = doc_cv.fit_transform(df_speeches_raw.speech)\n",
    "all_speeches_dtm = pd.DataFrame(_____.toarray(), columns=doc_cv.get_feature_names_out())\n",
    "len(all_speeches_dtm.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83224173",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    assert len(all_speeches_dtm.columns) == 111440\n",
    "    print(\"BoW korrekt erstellt!\")\n",
    "except AssertionError:\n",
    "    print(\"Die Variable `all_speeches_dtm` enthält nicht die korrekte Anzahl an Spalten\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b7f1c6b",
   "metadata": {},
   "source": [
    "4. Berechnen Sie nun die relative Dokumenthäufigkeit pro Wort und weisen Sie die 15 häufigsten Worte der Variable `most_common_words` zu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfad15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anzahl aller Dokumente\n",
    "count_docs = len(_____)\n",
    "\n",
    "# Kalkulation der relativen Dokumenthäufigkeit pro Token – Formel: die Anzahl der Dokumente / der Reden, in denen dieser Begriff auftritt, geteilt durch die Gesamtanzahl der Reden\n",
    "calc_docf = []\n",
    "\n",
    "# Iteration über alle Spalten\n",
    "for col in all_speeches_dtm.columns:\n",
    "    # Berechnung der relativen Dokumenthäufigkeit pro Token, indem die Summe aller Werte in einer Spalte durch die Anzahl aller Dokumente geteilt wird\n",
    "    word_docf = all_speeches_dtm[col].sum() / count_docs\n",
    "    calc_docf.append(_____)\n",
    "\n",
    "# Erstellung eines DataFrames aus der Liste\n",
    "df_docf = pd.DataFrame(calc_docf).transpose()\n",
    "# Umbenennung der Spaltennamen\n",
    "df_docf.columns = all_speeches_dtm._____\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21846440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausgabe der 15. häufigsten Tokens\n",
    "most_common_words = _____.transpose().sort_values(by=0, ascending=False).head(15)\n",
    "most_common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dc3f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    assert round(most_common_words.iloc[3].values[0]) == 1\n",
    "    print(\"Alles korrekt!\")\n",
    "except AssertionError:\n",
    "    print(\"Der Wert des Eintrags mit dem Index sollte gerundet 1 ergeben\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e3bd3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
